{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12800 files belonging to 2 classes.\n",
      "Found 3200 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "import os, pathlib, shutil, random\n",
    "from tensorflow import keras\n",
    "\n",
    "batch_size = 32\n",
    "base_dir = pathlib.Path('./input/aclImdb')\n",
    "val_dir = base_dir / \"val\"\n",
    "train_dir = base_dir / \"train\"\n",
    "for category in (\"neg\", \"pos\"):\n",
    "    os.makedirs(val_dir / category)\n",
    "    files = os.listdir(train_dir/ category)\n",
    "    random.Random(1337).shuffle(files)\n",
    "    num_val_samples = int(0.2 * (len(files)))\n",
    "    val_files = files[-num_val_samples:]\n",
    "    for fname in val_files:\n",
    "        shutil.move(train_dir / category / fname,\n",
    "                    val_dir / category / fname)\n",
    "\n",
    "train_ds = keras.utils.text_dataset_from_directory(\"./input/aclImdb/train\", batch_size=batch_size)\n",
    "val_ds = keras.utils.text_dataset_from_directory(\"./input/aclImdb/val\", batch_size=batch_size)\n",
    "test_ds = keras.utils.text_dataset_from_directory(\"./input/aclImdb/test\", batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'I must admit that I had my doubts about this movie before I was going to watch it. The main reason for that is because it was compared to a Hitchcock movie. I\\'ve seen several movies that were said to be inspired by Hitchcock or that could have been made by the \\'Master of Suspense\\' himself, but so far I haven\\'t seen any of these movie that would be able to stand the test of time. In my opinion Hitchcock has become a household name which is too easily used to promote some (cheap) thrillers, but on the other hand I must admit that I was intrigued by it because this is a European movie. Normally it\\'s the big Hollywood studios who like to abuse Hitchcock\\'s name if that can raise their income. But this movie was made in one of the most chauvinistic European countries ever and I\\'m sure that most French would rather drop dead than to admit that their movies have been inspired by an Englishman. That\\'s why I decided to give this movie a try and I must say that I\\'m glad that I did.<br /><br />\"Sur mes l\\xc3\\xa8vres\" or \"Read my Lips\" as it is called in English, tells the story of a young secretary named Carla. She is a hardworking and loyal employee, but has never been very appreciated by her colleagues. That has much to do with the fact that she suffers from a hearing deficiency, which has denied her to climb up on the hierarchical ladder of the company. But when she is allowed to hire a trainee that can work for her, all this is about to change. Paul Angeli is a 25 year old and completely unskilled ex-convict. The man is a thief, but Carla gives him a chance and covers for him when needed. She hopes to teach him what a regular life should look like, but at the same time he drags her with him in his old life...<br /><br />Since I still believe that the name Hitchcock is used too often to describe a very good thriller - which this movie definitely is - I will not make any comparisons between Hitchcock and Jacques Audiard\\'s directing. Fact is that the man has done a really good job with this movie. I hadn\\'t heard of him before, but it is true that he knows how to build up suspense and how to keep you interested from the beginning until the end. That also has a lot to do with the very fine and original story of course. I doubt if there is someone in Hollywood who has ever come up with the idea of using a handicapped woman in a powerful role, instead of making her the helpless subject of an abusive husband (you know, the typical TV-movie story).<br /><br />Also worth noticing is the acting in this movie. Vincent Cassel is quite famous, but Emmanuelle Devos was a complete mystery to me. There is absolutely nothing glamorous about their roles, but they both did an excellent job with their characters, making them feel very believable and realistic. Paul could have been the average tough guy right out of jail and Carla the typically helpless woman, but thanks to their performances, you really believe that these are two strong people who both have had some bad luck in life but who will make the best out of it together.<br /><br />All in all this is a powerful movie with a very fine script and some excellent acting. Despite the fact that I had my doubts about it, I\\'ve soon become one of its greatest admirers. I give this movie an 8/10. Don\\'t hesitate to give it a try.', shape=(), dtype=string)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for i in train_ds.take(1):\n",
    "    print(i[0][0])\n",
    "    print(i[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\malla\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import TextVectorization\n",
    "\n",
    "max_length = 600\n",
    "max_tokens = 20000\n",
    "text_vectorization = TextVectorization(max_tokens=max_tokens,output_mode=\"int\",output_sequence_length=max_length)\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "int_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "int_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "int_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Layer\n",
    "\n",
    "class TransformerEncoder(Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=self.embed_dim)\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [\n",
    "                keras.layers.Dense(dense_dim, activation='relu'),\n",
    "                keras.layers.Dense(embed_dim)\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm_1 = keras.layers.LayerNormalization()\n",
    "        self.layernorm_2 = keras.layers.LayerNormalization()\n",
    "        \n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "        # (batch, 600, 256)\n",
    "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output) # Residual attention\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_17 (InputLayer)       [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding_39 (Embedding)    (None, None, 256)         5120000   \n",
      "                                                                 \n",
      " transformer_encoder_16 (Tra  (None, None, 256)        543776    \n",
      " nsformerEncoder)                                                \n",
      "                                                                 \n",
      " global_max_pooling1d_5 (Glo  (None, 256)              0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,664,033\n",
      "Trainable params: 5,664,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 20000\n",
    "embed_dim = 256\n",
    "num_heads = 2\n",
    "dense_dim = 32\n",
    "\n",
    "inputs = keras.layers.Input(shape=(None,), dtype = \"int64\")\n",
    "x = keras.layers.Embedding(vocab_size, embed_dim)(inputs)\n",
    "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "# (b, 600,256)\n",
    "x = keras.layers.GlobalMaxPooling1D()(x)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# for i in int_train_ds:\n",
    "#     temp = keras.layers.Embedding(vocab_size, embed_dim)(i[0])\n",
    "#     print(temp.shape)\n",
    "#     print(model(i[0]).shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "400/400 [==============================] - 637s 2s/step - loss: 0.5010 - accuracy: 0.7545 - val_loss: 0.3660 - val_accuracy: 0.8353\n",
      "Epoch 2/3\n",
      "400/400 [==============================] - 676s 2s/step - loss: 0.3500 - accuracy: 0.8473 - val_loss: 0.3226 - val_accuracy: 0.8572\n",
      "Epoch 3/3\n",
      "400/400 [==============================] - 668s 2s/step - loss: 0.3039 - accuracy: 0.8704 - val_loss: 0.3224 - val_accuracy: 0.8581\n",
      "782/782 [==============================] - 2247s 3s/step - loss: 0.3374 - accuracy: 0.8532\n",
      "Test acc: 0.853\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"transformer_encoder.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=3, callbacks=callbacks)\n",
    "model = keras.models.load_model(\n",
    "    \"transformer_encoder.keras\",\n",
    "    custom_objects={\"TransformerEncoder\": TransformerEncoder})\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_54\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_69 (InputLayer)       [(None, None)]            0         \n",
      "                                                                 \n",
      " positional_embedding_50 (Po  (None, None, 256)        5273600   \n",
      " sitionalEmbedding)                                              \n",
      "                                                                 \n",
      " transformer_encoder_22 (Tra  (None, None, 256)        543776    \n",
      " nsformerEncoder)                                                \n",
      "                                                                 \n",
      " global_max_pooling1d_7 (Glo  (None, 256)              0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,817,633\n",
      "Trainable params: 5,817,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "400/400 [==============================] - 649s 2s/step - loss: 0.6189 - accuracy: 0.6884 - val_loss: 0.3923 - val_accuracy: 0.8194\n",
      "Epoch 2/2\n",
      "400/400 [==============================] - 705s 2s/step - loss: 0.3395 - accuracy: 0.8508 - val_loss: 0.3099 - val_accuracy: 0.8809\n",
      "782/782 [==============================] - 560s 715ms/step - loss: 0.3277 - accuracy: 0.8661\n",
      "Test acc: 0.866\n"
     ]
    }
   ],
   "source": [
    "class PositionalEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = keras.layers.Embedding(input_dim, output_dim)\n",
    "        self.position_embeddings = keras.layers.Embedding(sequence_length, output_dim)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=inputs.shape[-1], delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "    \n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "    \n",
    "    \n",
    "vocab_size = 20000\n",
    "sequence_length = 600\n",
    "embed_dim = 256\n",
    "num_heads = 2\n",
    "dense_dim = 32\n",
    "\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
    "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "x = keras.layers.GlobalMaxPooling1D()(x)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"full_transformer_encoder.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=2, callbacks=callbacks)\n",
    "model = keras.models.load_model(\n",
    "    \"full_transformer_encoder.keras\",\n",
    "    custom_objects={\"TransformerEncoder\": TransformerEncoder,\n",
    "                    \"PositionalEmbedding\": PositionalEmbedding})\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")\n",
    "\n",
    "# for i in int_train_ds:\n",
    "#     print(\"input shape\", i[0].shape)\n",
    "#     temp = keras.layers.Embedding(vocab_size, embed_dim)(i[0])\n",
    "#     print(temp.shape)\n",
    "#     print(model(i[0]).shape)\n",
    "#     break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs['english'].shape: (64, 20)\n",
      "inputs['spanish'].shape: (64, 20)\n",
      "targets.shape: (64, 20)\n"
     ]
    }
   ],
   "source": [
    "text_file = \"./input/spa-eng/spa.txt\"\n",
    "with open(text_file, encoding='utf-8') as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "text_pairs = []\n",
    "for line in lines:\n",
    "    english, spanish = line.split(\"\\t\")\n",
    "    spanish = \"[start] \" + spanish + \" [end]\"\n",
    "    text_pairs.append((english, spanish))\n",
    "    \n",
    "import random\n",
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples:]\n",
    "\n",
    "import tensorflow as tf\n",
    "import string\n",
    "import re\n",
    "import keras\n",
    "\n",
    "strip_chars = string.punctuation + \"¿\"\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(\n",
    "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
    "\n",
    "vocab_size = 15000\n",
    "sequence_length = 20\n",
    "\n",
    "source_vectorization = keras.layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "target_vectorization = keras.layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length + 1,\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "train_english_texts = [pair[0] for pair in train_pairs]\n",
    "train_spanish_texts = [pair[1] for pair in train_pairs]\n",
    "source_vectorization.adapt(train_english_texts)\n",
    "target_vectorization.adapt(train_spanish_texts)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "def format_dataset(eng, spa):\n",
    "    eng = source_vectorization(eng)\n",
    "    spa = target_vectorization(spa)\n",
    "    return ({\n",
    "        \"english\": eng,\n",
    "        \"spanish\": spa[:, :-1],\n",
    "    }, spa[:, 1:])\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    eng_texts, spa_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    spa_texts = list(spa_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs['english'].shape: (64, 20)\n",
      "inputs['spanish'].shape: (64, 20)\n",
      "targets.shape: (64, 20)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
    "    print(f\"inputs['spanish'].shape: {inputs['spanish'].shape}\")\n",
    "    print(f\"targets.shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
    "             layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        # (64, 20)\n",
    "        #print(\"input shape\", input_shape)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        #print(i.shape, j.shape)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        # (20, 20)\n",
    "        #print(mask.shape)\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        # (1, 20, 20)\n",
    "        #print(mask.shape)\n",
    "        # temp tf.Tensor([64], shape=(1,), dtype=int32)\n",
    "        # temp tf.Tensor([1 1], shape=(2,), dtype=int32)\n",
    "        # print(\"temp\",tf.expand_dims(batch_size, -1))\n",
    "        # print(\"temp\", tf.constant([1, 1], dtype=tf.int32))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1),\n",
    "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
    "        #print(\"mult\", mult)\n",
    "        # tf.Tensor([64  1  1], shape=(3,), dtype=int32)\n",
    "        # print(\"mult\", mult.shape)\n",
    "        # print(\"tile\", tf.tile(mask, mult).shape)\n",
    "        # (64, 20, 20)\n",
    "        return tf.tile(mask, mult)\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        #print(\"casual mask\", causal_mask.shape)\n",
    "        if mask is not None:\n",
    "            # casual mask (64, 20, 20)\n",
    "            # mask (64, 20)\n",
    "            #print(\"mask\", mask.shape)\n",
    "            padding_mask = tf.cast(\n",
    "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs,\n",
    "            value=inputs,\n",
    "            key=inputs,\n",
    "            attention_mask=causal_mask)\n",
    "        # print(\"attention_output_1\", attention_output_1.shape)\n",
    "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=attention_output_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "        )\n",
    "        attention_output_2 = self.layernorm_2(\n",
    "            attention_output_1 + attention_output_2)\n",
    "        # attention_output_2 (64, 20, 256)\n",
    "        # print(\"attention_output_2\", attention_output_2.shape)\n",
    "        proj_output = self.dense_proj(attention_output_2)\n",
    "        # proj_output (64, 20, 256)\n",
    "        return self.layernorm_3(attention_output_2 + proj_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_outputs (None, None, 256)\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 256\n",
    "dense_dim = 2048\n",
    "num_heads = 8\n",
    "vocab_size = 15000\n",
    "sequence_length = 20\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "# encoder_outputs (64, 20, 256)\n",
    "print(\"encoder_outputs\", encoder_outputs.shape)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "\n",
    "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# for i in train_ds:\n",
    "#     print(\"input shape\", i[0]['english'].shape)\n",
    "#     #print(temp.shape)\n",
    "#     print(transformer([i[0]['english'],i[0]['spanish']]).shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "transformer.fit(train_ds, epochs=1, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 20)\n",
      "tf.Tensor(\n",
      "[[  19   22 1330 ...    0    0    0]\n",
      " [ 106 8006   40 ...    0    0    0]\n",
      " [ 692    7 6554 ...    0    0    0]\n",
      " ...\n",
      " [   7   78  128 ...    0    0    0]\n",
      " [  25  276    4 ...    0    0    0]\n",
      " [  67 7170    3 ...    0    0    0]], shape=(64, 20), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "spa_vocab = target_vectorization.get_vocabulary()\n",
    "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
    "max_decoded_sentence_length = 20\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = target_vectorization(\n",
    "            [decoded_sentence])[:, :-1]\n",
    "        predictions = transformer(\n",
    "            [tokenized_input_sentence, tokenized_target_sentence])\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = spa_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "for _ in range(20):\n",
    "    input_sentence = random.choice(test_eng_texts)\n",
    "    print(\"-\")\n",
    "    print(input_sentence)\n",
    "    print(decode_sequence(input_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c1a4f09e9b128b35def6df724a69ee0e8c626d8a51d7987c9a1415caa06c9ece"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
